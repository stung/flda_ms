\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multicol}
\usepackage{geometry}
 \geometry{
 a4paper,
 left=15mm,
 right=15mm,
 top=20mm,
 bottom=20mm,
 }

\title{UCLA Computer Science Master's Comprehensive Exam: Implementation and Extension of Followship-LDA}

\author{Spencer Tung \\ Advisor: Professor Junghoo Cho}

\date{\today}

\begin{document}
\maketitle

\begin{multicols}{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
asdf
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Section 1: Project Idea%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Semantic analysis, the art of getting computers and programs to understand the meaning behind text, has been a long standing problem in computer science. There can be many practical applications of semantic analysis, such as identifying the topics that comprise a given document. This, in turn, could be used to determine what sort of topics the reader of this document is potentially be interested in. In today's advertisement-driven and consumer-focused world, being able to automatically obtain this information, independent of the source, is invaluable.

These days, there is a large amount of traffic centered around social media, which many advertisers leverage to market their products. Twitter is one such social media site, focused on microblogs of 140 characters or less. Every time a Twitter user submits a post, known as a \textit{tweet}, the tweet is blasted out to all of the other users that have chosen to \textit{follow} this specific user. The information flow is therefore generated by the \textit{followee} and received by the \textit{follower}. On a microblog site such as Twitter, it can be difficult for marketers to determine where their ads will have the most weight, as there are literally millions of tweets coming from millions of users every day \cite{TODO}. Knowing who key influencers are on Twitter goes a long way for advertisers, who can be more selective about how they reach out to their audience.

However, there are some advertisers who, instead of taking the time to do their own research and fine tune their efforts, will resort to more widespread methods to attract followers. This will often take the form of a computer scipt that blasts their advertisements to anyone who follows them. Known as \textit{Twitterbots}, or spambots to most, these drones are often viewed as being annoyances to legitimate users who may not care for the bot tweets. It is therefore advantageous to be able to automatically identify whether a user is legitimate or a bot and take appropriate measures accordingly.

This paper will focus on an extension of \textit{Latent Dirichlet Analysis} (LDA). LDA is a probabilistic topic model, and can be applied to a set of documents (known as a \textit{corpus}) to extract topic information for each document and every word in a given corpus. The extension of this model, known as \textit{Followship-LDA} (FLDA), leverages Twitter follower-followee network information to identify either important and influential entities on Twitter, or spambots that most people won't pay attention to and should remove from their followee list. The details of both models will be discussed in section \ref{sec:prevwork}.

The paper will be organized as follows. Section \ref{sec:prevwork} will discuss any previous work that came before this paper. Most importantly, it details the FLDA method critical to algorithm's success. Section \ref{sec:approach} will discuss the system architecture and design. Section \ref{sec:results} will detail the results of running the FLDA model on the Twitter data subset. Finally, section \ref{sec:conc} will discuss the conclusions.

\subsection{Motivation}
I chose this project to further my understanding of semantic analysis, as well as challenge myself by implementing an algorithm that could efficiently deal with big data.


%%%%%%%%%%%%%%%%%%%%%%%% Section 2: Methodology and Previous Work%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology and Previous Work}
\label{sec:prevwork}
\subsection{Background}
One approach to deriving meaning behind the text is through topic models. These are models that operate under the following assumptions: that a document is a mixture of different topics, and each topic is a distribution of words. As these are \textit{generative models} for documents, it demonstrates a probabilistic procedure that can be used to generate various documents. Using statistical techniques, it is possible to invert this process and infer the set of topics that were used to generate the document in question. 

To understand how FLDA works, it is necessary to first gain an understanding of how LDA works. As mentioned previously, LDA is a method to statistically 
\cite{lda}. 

\subsection{Previous Work}
The primary source of information came from Professor John Cho's paper on FLDA \cite{flda}. 

To solve this, a group of researchers proposed a new LDA model - one that could take into account these connections from the Twitter network. This is known as Followship-LDA (FLDA)

%%%%%%%%%%%%%%%%%%%%%%%% Section 3: System Design and Approaches%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Design and Approaches}
\label{sec:approach}
\subsection{System Architecture Overview}
Given that LDA is already a well known model, there are many existing open source implementations readily available. I chose to use the gibbsLDA \verb!C++! implementation \cite{gibbs_lda}, understand the code, and use that as a baseline from which to extend and model the followship network analysis from. 

To narrow the scope of my project, I decided to focus only on implementing a working version of the FLDA described in 

Having defined the scope of my project, I 
located an implementation of  \cite{gibbs_lda}
% \begin{figure}
%   \centering
%     \includegraphics[width=0.7\textwidth]{architecture}
%   \caption {System Architecture }
%   \label{fig:architecture}
% \end{figure}
% Figure \ref{fig:architecture} shows a visual overview of our system. We obtain our data from our sources, Rotten Tomatoes, Box Office Mojo, and IMDb, then store that in a number of databases. We then used the data to create a regression model on the data, and then use that model to predict the box office results based on the features of our choice.

\subsection{Data Sources}
Twitter was chosen as the microblog in question.

\subsection{Data Cleaning and Challenges}
I obtained the data from Zijun, which he described as a subset of the 

I worked with another student on cleaning the tweets from each individual user. I removed usernames 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Section 4: Set Up and Results%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}
\subsection{Preliminary Results}
As I was extending an existing software package, I needed to ensure that the original program would reliably run LDA on a modified dataset.

% \begin{table} [h]
% \begin{center}
% \begin{tabular}{ l | l | l | l | l | l }
%   Intercept & V1 (CriticScore) & V2 (AudScore) & V3 (StarRank) & V4 (Year) & V5 (NumTheaters) \\
%   \hline
%   -0.20523 & 0.04814 & 0.09007 & 0.04626 & -0.07458 & 0.45317 \\
% \end{tabular}
% \caption{Coefficients for the opening weekend gross linear model}
% \label{table:openlin}
% \end{center}
% \end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Section 5: Evaluation and Discussion%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}
\label{sec:conc}

As described by Bi et al. in \cite{flda}, it is possible to implement FLDA in such a way that allows for parallelization. The implementation done by their group leveraged Spark, a framework that 

As Spark comes with built in Java support, it would have required a different approach to 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Section 6: Teamwork%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Miscellaneous}
\label{sec:misc}
\subsection{Programming Languages}
Two programming languages were used in this project:
\begin{enumerate}
\item \textbf{Python}: 
\item \textbf{C++}: 
\end{enumerate}

\end{multicols}

\begin{thebibliography}{9}
% APA Style
\bibitem{flda}
Bi, B., Tian, Y., Sismanis, Y., Balmin, A., \& Cho, J. (2014, February). Scalable topic-specific influence analysis on microblogs. In \textit{Proceedings of the 7th ACM international conference on Web search and data mining} (pp. 513-522). ACM.

\bibitem{lda}
Steyvers, M., \& Griffiths, T. (2007). Probabilistic topic models. \textit{Handbook of latent semantic analysis}, 427(7), 424-440.

\bibitem{gibbs_lda}
Xuan-Hieu Phan and Cam-Tu Nguyen. GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA), 2007

\bibitem{TODO}
YOU NEED TO CITE ALL OF THESE SOURCES

\end{thebibliography}

\end{document}